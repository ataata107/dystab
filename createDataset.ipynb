{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "createDataset.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNM7VtZbXUo4ArYjaK9X1eS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ataata107/dystab/blob/master/createDataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf13TNY812eY"
      },
      "source": [
        "!git clone https://github.com/philferriere/tfoptflow.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i_jx8qVX18TN"
      },
      "source": [
        "%cd tfoptflow/tfoptflow/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9co-eviN2CYA"
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.13.2\n",
        "!pip uninstall tensorflow-gpu\n",
        "!pip install tensorflow-gpu==1.13.2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZiwlchA2NhK"
      },
      "source": [
        "Download the FBMS dataset and place it in this folder to be zipped in next step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kV5LvRJk2DEP"
      },
      "source": [
        "!unzip FBMS_Trainingset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ia8wJQF2b8j"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "from copy import deepcopy\n",
        "from skimage.io import imread,imsave\n",
        "from skimage.transform import rescale, resize, downscale_local_mean\n",
        "from model_pwcnet import ModelPWCNet, _DEFAULT_PWCNET_TEST_OPTIONS\n",
        "from visualize import display_img_pairs_w_flows\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saXY7kSu2gKa"
      },
      "source": [
        "# TODO: Set device to use for inference\n",
        "# Here, we're using a GPU (use '/device:CPU:0' to run inference on the CPU)\n",
        "gpu_devices = ['/device:GPU:0']  \n",
        "controller = '/device:GPU:0'\n",
        "\n",
        "# TODO: Set the path to the trained model (make sure you've downloaded it first from http://bit.ly/tfoptflow)\n",
        "ckpt_path = './models/pwcnet-lg-6-2-multisteps-chairsthingsmix/pwcnet.ckpt-595000'\n",
        "\n",
        "# Configure the model for inference, starting with the default options\n",
        "nn_opts = deepcopy(_DEFAULT_PWCNET_TEST_OPTIONS)\n",
        "nn_opts['verbose'] = True\n",
        "nn_opts['ckpt_path'] = ckpt_path\n",
        "nn_opts['batch_size'] = 1\n",
        "nn_opts['gpu_devices'] = gpu_devices\n",
        "nn_opts['controller'] = controller\n",
        "\n",
        "# We're running the PWC-Net-large model in quarter-resolution mode\n",
        "# That is, with a 6 level pyramid, and upsampling of level 2 by 4 in each dimension as the final flow prediction\n",
        "nn_opts['use_dense_cx'] = True\n",
        "nn_opts['use_res_cx'] = True\n",
        "nn_opts['pyr_lvls'] = 6\n",
        "nn_opts['flow_pred_lvl'] = 2\n",
        "\n",
        "# The size of the images in this dataset are not multiples of 64, while the model generates flows padded to multiples\n",
        "# of 64. Hence, we need to crop the predicted flows to their original size\n",
        "nn_opts['adapt_info'] = (1, 436, 1024, 2)\n",
        "\n",
        "# Instantiate the model in inference mode and display the model configuration\n",
        "nn = ModelPWCNet(mode='test', options=nn_opts)\n",
        "nn.print_config()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StVUJmHn2nX_"
      },
      "source": [
        "import os\n",
        "import cv2\n",
        "img_dirs = sorted(os.listdir(\"./Trainingset/\"))\n",
        "print(img_dirs)\n",
        "if not os.path.exists('./Dataset'):\n",
        "  os.makedirs('./Dataset')\n",
        "if not os.path.exists('./Dataset/image_pair'):\n",
        "    os.makedirs('./Dataset/image_pair')\n",
        "if not os.path.exists('./Dataset/flow_pair'):\n",
        "    os.makedirs('./Dataset/flow_pair')\n",
        "if not os.path.exists('./Dataset/flows'):\n",
        "    os.makedirs('./Dataset/flows')\n",
        "counter = 0\n",
        "for i in img_dirs:\n",
        "  print(i)\n",
        "  \n",
        "  imgs = sorted(os.listdir(\"./Trainingset/\"+i+\"/\"))\n",
        "  imgs = (list(filter(lambda x: x!=\"GroundTruth\" and not x.endswith(\".bmf\"), imgs)))\n",
        "  # print(len(imgs))\n",
        "  for j in range(len(imgs)-3):\n",
        "    img_pairs = []\n",
        "    img_names = []  \n",
        "    img_pairs_back = []\n",
        "    image_path1 = \"./Trainingset/\"+i+\"/\" + imgs[j]\n",
        "    image_path2 = \"./Trainingset/\"+i+\"/\" + imgs[j+1]\n",
        "    image1, image2 = cv2.imread(image_path1), cv2.imread(image_path2)\n",
        "    image1, image2 = cv2.resize(image1, (256, 256)),cv2.resize(image2, (256, 256))\n",
        "\n",
        "    img_pairs.append((image1, image2))\n",
        "    img_pairs_back.append((image2, image1))\n",
        "    img_names.append((imgs[j], imgs[j+1]))\n",
        "\n",
        "    image_path2 = \"./Trainingset/\"+i+\"/\" + imgs[j+2]\n",
        "    image2 = cv2.imread(image_path2)\n",
        "    image2 = cv2.resize(image2, (256, 256))\n",
        "    img_pairs.append((image1, image2))\n",
        "    img_pairs_back.append((image2, image1))\n",
        "    img_names.append((imgs[j], imgs[j+2]))\n",
        "\n",
        "    image_path2 = \"./Trainingset/\"+i+\"/\" + imgs[j+3]\n",
        "    image2 = cv2.imread(image_path2)\n",
        "    image2 = cv2.resize(image2, (256, 256))\n",
        "    img_pairs.append((image1, image2))\n",
        "    img_pairs_back.append((image2, image1))\n",
        "    img_names.append((imgs[j], imgs[j+3]))\n",
        "\n",
        "\n",
        "    \n",
        "    pred_labels_front = nn.predict_from_img_pairs(img_pairs, batch_size=1, verbose=False)\n",
        "    pred_labels_back = nn.predict_from_img_pairs(img_pairs_back, batch_size=1, verbose=False)\n",
        "    for k in range(len(img_pairs)):\n",
        "      if not os.path.exists('./Dataset/image_pair/'+ str(k+counter)):\n",
        "        os.makedirs('./Dataset/image_pair/'+ str(k+counter))\n",
        "      if not os.path.exists('./Dataset/flow_pair/'+ str(k+counter)):\n",
        "        os.makedirs('./Dataset/flow_pair/'+ str(k+counter))\n",
        "      cv2.imwrite(\"./Dataset/image_pair/\"+ str(k+counter) + \"/\" + img_names[k][0],img_pairs[k][0])\n",
        "      cv2.imwrite(\"./Dataset/image_pair/\"+ str(k+counter) + \"/\" + img_names[k][1],img_pairs[k][1])\n",
        "      np.save(\"./Dataset/flow_pair/\"+ str(k+counter) + \"/forward\", pred_labels_front[k])\n",
        "      np.save(\"./Dataset/flow_pair/\"+ str(k+counter) + \"/backward\", pred_labels_back[k])\n",
        "    \n",
        "      if not os.path.exists('./Dataset/flows/'+ str(k+counter)):\n",
        "        os.makedirs('./Dataset/flows/'+ str(k+counter))\n",
        "      for o,l in enumerate(pred_labels_front):\n",
        "        np.save(\"./Dataset/flows/\"+ str(k+counter) + \"/\"+str(o), l)\n",
        "    counter+=3\n",
        "    print(counter)\n",
        "  \n",
        "    \n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayrEcXX-2z7s"
      },
      "source": [
        "Now move the Dataset folder created here to the dystab root folder"
      ]
    }
  ]
}